{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1111676,"sourceType":"datasetVersion","datasetId":623289}],"dockerImageVersionId":30646,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pickle\nimport numpy as np\nimport tensorflow as tf\nfrom tqdm.notebook import tqdm\nfrom tensorflow.keras.applications.vgg16 import VGG16,preprocess_input\nfrom tensorflow.keras.preprocessing.image import load_img,img_to_array\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.utils import to_categorical,plot_model\nfrom tensorflow.keras.layers import Input,Dense,LSTM,Embedding,Dropout,add","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-05T12:48:05.482542Z","iopub.execute_input":"2024-02-05T12:48:05.483362Z","iopub.status.idle":"2024-02-05T12:48:05.491195Z","shell.execute_reply.started":"2024-02-05T12:48:05.483319Z","shell.execute_reply":"2024-02-05T12:48:05.489964Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"BASE_DIR = '/kaggle/input/flickr8k'\nWORKING_DIR = '/kaggle/working'","metadata":{"execution":{"iopub.status.busy":"2024-02-05T12:48:26.917318Z","iopub.execute_input":"2024-02-05T12:48:26.917752Z","iopub.status.idle":"2024-02-05T12:48:26.923145Z","shell.execute_reply.started":"2024-02-05T12:48:26.917717Z","shell.execute_reply":"2024-02-05T12:48:26.921740Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Extract Image Feature","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.applications.vgg16 import VGG16\nfrom tensorflow.keras.models import Model\n\n# Path to the manually downloaded weights file\nweights_path = 'C:\\\\Users\\\\hp\\\\.keras\\\\models'\n\n# Load VGG16 model with the specified weights file\nmodel = VGG16(weights=None)\n\n# Restructure the model\nmodel = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n\n# Summary\nprint(model.summary())\n","metadata":{"execution":{"iopub.status.busy":"2024-02-05T13:18:43.766593Z","iopub.execute_input":"2024-02-05T13:18:43.767555Z","iopub.status.idle":"2024-02-05T13:18:45.543591Z","shell.execute_reply.started":"2024-02-05T13:18:43.767514Z","shell.execute_reply":"2024-02-05T13:18:45.542376Z"},"trusted":true},"execution_count":54,"outputs":[{"name":"stdout","text":"Model: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n                                                                 \n block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n                                                                 \n block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n                                                                 \n block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n                                                                 \n block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n                                                                 \n block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n                                                                 \n block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n                                                                 \n block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n                                                                 \n block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n                                                                 \n block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n                                                                 \n block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n                                                                 \n block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n                                                                 \n block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n                                                                 \n block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n                                                                 \n block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n                                                                 \n block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n                                                                 \n block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n                                                                 \n block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n                                                                 \n block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n                                                                 \n flatten (Flatten)           (None, 25088)             0         \n                                                                 \n fc1 (Dense)                 (None, 4096)              102764544 \n                                                                 \n fc2 (Dense)                 (None, 4096)              16781312  \n                                                                 \n=================================================================\nTotal params: 134260544 (512.16 MB)\nTrainable params: 134260544 (512.16 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n","output_type":"stream"}]},{"cell_type":"code","source":"features={}\ndirectory = os.path.join(BASE_DIR,'Images')\n\nfor img_name in tqdm(os.listdir(directory)):\n    img_path = directory + '/' + img_name\n    image = load_img(img_path , target_size = (224,224))\n    \n    image = img_to_array(image)\n    \n    image=image.reshape((1,image.shape[0],image.shape[1],image.shape[2]))\n    image = preprocess_input(image)\n    feature = model.predict(image, verbose =0)\n    image_id = img_name.split('.')[0]\n    features[image_id] = feature\n","metadata":{"execution":{"iopub.status.busy":"2024-02-05T13:19:21.602356Z","iopub.execute_input":"2024-02-05T13:19:21.603016Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/8091 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69c3f02b43a547b19f308c14a02c1c70"}},"metadata":{}}]},{"cell_type":"code","source":"pickle.dump(features,open(os.path.join(WORKING_DIR, 'features.pkl'),'wb'))","metadata":{"execution":{"iopub.status.busy":"2024-02-04T04:09:09.373024Z","iopub.status.idle":"2024-02-04T04:09:09.373467Z","shell.execute_reply.started":"2024-02-04T04:09:09.373257Z","shell.execute_reply":"2024-02-04T04:09:09.373275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(os.path.join(WORKING_DIR,'features.pkl'),'rb') as f:\n    features=pickle.load(f)","metadata":{"execution":{"iopub.status.busy":"2024-02-04T04:09:09.375311Z","iopub.status.idle":"2024-02-04T04:09:09.375695Z","shell.execute_reply.started":"2024-02-04T04:09:09.375510Z","shell.execute_reply":"2024-02-04T04:09:09.375525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load the Captions Data","metadata":{}},{"cell_type":"code","source":"with open(os.path.join(BASE_DIR,'captions.txt'),'r') as f:\n    next(f)\n    captions_doc=f.read()","metadata":{"execution":{"iopub.status.busy":"2024-02-05T13:10:12.558494Z","iopub.execute_input":"2024-02-05T13:10:12.558893Z","iopub.status.idle":"2024-02-05T13:10:12.629889Z","shell.execute_reply.started":"2024-02-05T13:10:12.558848Z","shell.execute_reply":"2024-02-05T13:10:12.628981Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"mapping = {}\n#process\nfor line  in tqdm(captions_doc.split('\\n')):\n    tokens=line.split(',')\n    if len(line)<2:\n        continue\n    image_id,caption = tokens[0],tokens[1:]\n    image_id = image_id.split('.')[0]\n    caption = \" \".join(caption)\n    if image_id not in mapping:\n        mapping[image_id] = []\n    mapping[image_id].append(caption)    \n        \n    ","metadata":{"execution":{"iopub.status.busy":"2024-02-05T13:10:17.411546Z","iopub.execute_input":"2024-02-05T13:10:17.412457Z","iopub.status.idle":"2024-02-05T13:10:17.582590Z","shell.execute_reply.started":"2024-02-05T13:10:17.412424Z","shell.execute_reply":"2024-02-05T13:10:17.581254Z"},"trusted":true},"execution_count":30,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/40456 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5cc67616df74f68b59d14d364308673"}},"metadata":{}}]},{"cell_type":"code","source":"len(mapping)","metadata":{"execution":{"iopub.status.busy":"2024-02-05T09:37:22.540371Z","iopub.execute_input":"2024-02-05T09:37:22.540759Z","iopub.status.idle":"2024-02-05T09:37:22.548557Z","shell.execute_reply.started":"2024-02-05T09:37:22.540729Z","shell.execute_reply":"2024-02-05T09:37:22.547438Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"8091"},"metadata":{}}]},{"cell_type":"markdown","source":"## Preprocessing","metadata":{}},{"cell_type":"code","source":"def clean (mapping):\n    for key,captions in mapping.items():\n        for i in range(len(captions)):\n            caption=captions[i]\n            #convert to lowercase\n            caption=caption.lower()\n            #delete digits , special characters etc\n            caption=caption.replace('[^A-Za-z]','')\n            #delete additional spaces\n            caption=caption.replace('\\s+', ' ')\n            #add start and end tags\n            caption = \"start\" + \" \"+ \" \".join([word for word in caption.split() if len(word)>1]) + \" \"+ \"end\"\n            captions[i]=caption\n            \n            \n            \n            ","metadata":{"execution":{"iopub.status.busy":"2024-02-05T13:12:44.602685Z","iopub.execute_input":"2024-02-05T13:12:44.603093Z","iopub.status.idle":"2024-02-05T13:12:44.610865Z","shell.execute_reply.started":"2024-02-05T13:12:44.603051Z","shell.execute_reply":"2024-02-05T13:12:44.609812Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"mapping['1000268201_693b08cb0e']","metadata":{"execution":{"iopub.status.busy":"2024-02-05T13:12:50.347653Z","iopub.execute_input":"2024-02-05T13:12:50.348205Z","iopub.status.idle":"2024-02-05T13:12:50.356515Z","shell.execute_reply.started":"2024-02-05T13:12:50.348163Z","shell.execute_reply":"2024-02-05T13:12:50.354264Z"},"trusted":true},"execution_count":40,"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"['A child in a pink dress is climbing up a set of stairs in an entry way .',\n 'A girl going into a wooden building .',\n 'A little girl climbing into a wooden playhouse .',\n 'A little girl climbing the stairs to her playhouse .',\n 'A little girl in a pink dress going into a wooden cabin .']"},"metadata":{}}]},{"cell_type":"code","source":"clean(mapping)","metadata":{"execution":{"iopub.status.busy":"2024-02-05T13:12:56.117093Z","iopub.execute_input":"2024-02-05T13:12:56.117520Z","iopub.status.idle":"2024-02-05T13:12:56.306691Z","shell.execute_reply.started":"2024-02-05T13:12:56.117484Z","shell.execute_reply":"2024-02-05T13:12:56.305669Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"id = mapping['1000268201_693b08cb0e']","metadata":{"execution":{"iopub.status.busy":"2024-02-05T09:38:18.070597Z","iopub.execute_input":"2024-02-05T09:38:18.071724Z","iopub.status.idle":"2024-02-05T09:38:18.076633Z","shell.execute_reply.started":"2024-02-05T09:38:18.071665Z","shell.execute_reply":"2024-02-05T09:38:18.075506Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"img = mpimg.imread(id)\nplt.imshow(img)\nplt.axis('off')  # Turn off axis labels\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_captions = []\nfor key in mapping:\n    for caption in mapping[key]:\n        all_captions.append(caption)","metadata":{"execution":{"iopub.status.busy":"2024-02-05T13:10:36.227175Z","iopub.execute_input":"2024-02-05T13:10:36.227558Z","iopub.status.idle":"2024-02-05T13:10:36.242315Z","shell.execute_reply.started":"2024-02-05T13:10:36.227528Z","shell.execute_reply":"2024-02-05T13:10:36.240975Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"len(all_captions)","metadata":{"execution":{"iopub.status.busy":"2024-02-03T18:35:08.258088Z","iopub.execute_input":"2024-02-03T18:35:08.258524Z","iopub.status.idle":"2024-02-03T18:35:08.266494Z","shell.execute_reply.started":"2024-02-03T18:35:08.258489Z","shell.execute_reply":"2024-02-03T18:35:08.265182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_captions[:10]","metadata":{"execution":{"iopub.status.busy":"2024-02-05T13:13:00.846979Z","iopub.execute_input":"2024-02-05T13:13:00.847827Z","iopub.status.idle":"2024-02-05T13:13:00.855543Z","shell.execute_reply.started":"2024-02-05T13:13:00.847782Z","shell.execute_reply":"2024-02-05T13:13:00.854266Z"},"trusted":true},"execution_count":42,"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"['A child in a pink dress is climbing up a set of stairs in an entry way .',\n 'A girl going into a wooden building .',\n 'A little girl climbing into a wooden playhouse .',\n 'A little girl climbing the stairs to her playhouse .',\n 'A little girl in a pink dress going into a wooden cabin .',\n 'A black dog and a spotted dog are fighting',\n 'A black dog and a tri-colored dog playing with each other on the road .',\n 'A black dog and a white dog with brown spots are staring at each other in the street .',\n 'Two dogs of different breeds looking at each other on the road .',\n 'Two dogs on pavement moving toward each other .']"},"metadata":{}}]},{"cell_type":"code","source":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(all_captions)\nvocab_size = len(tokenizer.word_index) + 1","metadata":{"execution":{"iopub.status.busy":"2024-02-05T09:55:37.135929Z","iopub.execute_input":"2024-02-05T09:55:37.136348Z","iopub.status.idle":"2024-02-05T09:55:37.949013Z","shell.execute_reply.started":"2024-02-05T09:55:37.136316Z","shell.execute_reply":"2024-02-05T09:55:37.948162Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"vocab_size","metadata":{"execution":{"iopub.status.busy":"2024-02-03T18:39:18.758424Z","iopub.execute_input":"2024-02-03T18:39:18.758873Z","iopub.status.idle":"2024-02-03T18:39:18.766404Z","shell.execute_reply.started":"2024-02-03T18:39:18.758839Z","shell.execute_reply":"2024-02-03T18:39:18.765228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_length = max(len(caption.split()) for caption in all_captions)\nmax_length","metadata":{"execution":{"iopub.status.busy":"2024-02-05T09:55:42.551365Z","iopub.execute_input":"2024-02-05T09:55:42.551787Z","iopub.status.idle":"2024-02-05T09:55:42.598293Z","shell.execute_reply.started":"2024-02-05T09:55:42.551753Z","shell.execute_reply":"2024-02-05T09:55:42.597177Z"},"trusted":true},"execution_count":43,"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"33"},"metadata":{}}]},{"cell_type":"markdown","source":"## Train Test Split","metadata":{}},{"cell_type":"code","source":"image_ids=list(mapping.keys())\nsplit = int(len(image_ids)*0.90)\ntrain = image_ids[:split]\ntest=image_ids[split:]","metadata":{"execution":{"iopub.status.busy":"2024-02-05T09:55:45.436029Z","iopub.execute_input":"2024-02-05T09:55:45.436462Z","iopub.status.idle":"2024-02-05T09:55:45.442759Z","shell.execute_reply.started":"2024-02-05T09:55:45.436408Z","shell.execute_reply":"2024-02-05T09:55:45.441479Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"def data_generator(data_keys, mapping, features, tokenizer, max_length, vocab_size, batch_size):\n    n = 0\n    X1, X2, y = None, None, []\n    \n    while True:\n        for key in data_keys:\n            n += 1\n            captions = mapping[key]\n\n            for caption in captions:\n                seq = tokenizer.texts_to_sequences([caption])[0]\n\n                for i in range(1, len(seq)):\n                    in_seq, out_seq = seq[:i], seq[i]\n                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n\n                    if X1 is None:\n                        X1 = np.array([features[key][0]])\n                    else:\n                        X1 = np.concatenate([X1, np.array([features[key][0]])], axis=0)\n\n                    in_seq = np.expand_dims(in_seq, axis=0)\n\n                    if X2 is not None and X2.shape[1] == in_seq.shape[1]:\n                        X2 = np.vstack([X2, in_seq])\n                    else:\n                        X2 = in_seq\n\n                    y.append(out_seq)\n\n            if n == batch_size:\n                yield [X1, X2], np.array(y)\n                X1, X2, y = None, None, []\n                n = 0\n","metadata":{"execution":{"iopub.status.busy":"2024-02-05T09:55:18.730875Z","iopub.execute_input":"2024-02-05T09:55:18.731374Z","iopub.status.idle":"2024-02-05T09:55:18.743789Z","shell.execute_reply.started":"2024-02-05T09:55:18.731336Z","shell.execute_reply":"2024-02-05T09:55:18.742692Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"## Model Creation","metadata":{}},{"cell_type":"code","source":"#save\nmodel.save(WORKING_DIR+'/best_model.h5')","metadata":{"execution":{"iopub.status.busy":"2024-02-05T13:18:54.973270Z","iopub.execute_input":"2024-02-05T13:18:54.973685Z","iopub.status.idle":"2024-02-05T13:18:55.729986Z","shell.execute_reply.started":"2024-02-05T13:18:54.973650Z","shell.execute_reply":"2024-02-05T13:18:55.728784Z"},"trusted":true},"execution_count":55,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n  saving_api.save_model(\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Generate Captions for the Image","metadata":{}},{"cell_type":"code","source":"def idx_to_word(integer,tokenizer):\n    for word,index in tokenizer.word_index.items():\n        if index == integer:\n            return word\n    return none    ","metadata":{"execution":{"iopub.status.busy":"2024-02-05T09:55:05.550641Z","iopub.execute_input":"2024-02-05T09:55:05.551041Z","iopub.status.idle":"2024-02-05T09:55:05.556910Z","shell.execute_reply.started":"2024-02-05T09:55:05.551012Z","shell.execute_reply":"2024-02-05T09:55:05.555660Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"def index_to_word(index, tokenizer):\n    for word, idx in tokenizer.word_index.items():\n        if idx == index:\n            return word","metadata":{"execution":{"iopub.status.busy":"2024-02-05T09:55:08.355790Z","iopub.execute_input":"2024-02-05T09:55:08.356725Z","iopub.status.idle":"2024-02-05T09:55:08.361651Z","shell.execute_reply.started":"2024-02-05T09:55:08.356675Z","shell.execute_reply":"2024-02-05T09:55:08.360763Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"def predict_caption(model, image, tokenizer, max_length):\n    in_text='start'\n    for i in range(max_length):\n        sequence=tokenizer.texts_to_sequences([in_text])[0]\n        sequence=pad_sequences([sequence],max_length)[0]\n        yhat=model.predict([image,sequence], verbose=0)\n        yhat=np.argmax(yhat)\n        word=idx_to_word(yhat,tokenizer)\n        if word is None:\n            break\n        in_text += \" \"+ word\n        if word == 'end':\n            break\n    return in_text       \n        \n    \n       \n        \n    ","metadata":{"execution":{"iopub.status.busy":"2024-02-05T13:11:03.727793Z","iopub.execute_input":"2024-02-05T13:11:03.728193Z","iopub.status.idle":"2024-02-05T13:11:03.736267Z","shell.execute_reply.started":"2024-02-05T13:11:03.728164Z","shell.execute_reply":"2024-02-05T13:11:03.735015Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"from nltk.translate.bleu_score import corpus_bleu\nactual,predicted = list(),list()\n\nfor key in tqdm(test):\n    captions=mapping[key]\n    y_pred=predict_caption(model,features[key],tokenizer,max_length)\n    actual_captions=[caption.split() for caption in captions]\n    y_pred=y_pred.split()\n    actual.append(actual_captions)\n    predicted.append(y_pred)\n    \nprint(\"BLEU-1: %f\" % corpus_bleu(actual,predicted,weights=(1.0,0,0,0)))\nprint(\"BLEU-2: %f\" % corpus_bleu(actual,predicted,weights=(0.5,0.5,0,0)))","metadata":{"execution":{"iopub.status.busy":"2024-02-04T02:21:11.628921Z","iopub.execute_input":"2024-02-04T02:21:11.631288Z","iopub.status.idle":"2024-02-04T03:12:23.070726Z","shell.execute_reply.started":"2024-02-04T02:21:11.631213Z","shell.execute_reply":"2024-02-04T03:12:23.069310Z"},"trusted":true},"execution_count":152,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/810 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0db23c42b59543dba2f48324da48b70a"}},"metadata":{}},{"name":"stdout","text":"BLEU-1: 0.191293\nBLEU-2: 0.099497\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Visualization","metadata":{}},{"cell_type":"code","source":"from PIL import Image\nimport matplotlib.pyplot as plt\n\n\ndef get_captions_for_image(image_name):\n    # Your code to retrieve captions for the image\n    # Replace this with your actual implementation\n    return [\"Caption 1\", \"Caption 2\", \"Caption 3\"]\n\ndef generate_captions(image_name):\n    img_path = '/kaggle/input/flickr8k/Images/' + image_name\n    image = Image.open(img_path)\n    \n    # Get captions for the image\n    captions = get_captions_for_image(image_name)\n    \n    print(\"---------------Actual----------------------\")\n    for caption in captions:\n        print(caption)\n    \n    # Assuming you have a function to predict captions\n    y_pred = predict_caption(model, features[image_id], tokenizer, max_length)\n    print(y_pred)\n    plt.imshow(image)\n\n    \n    \n  \n    \n    \n    \n","metadata":{"execution":{"iopub.status.busy":"2024-02-05T13:17:38.607757Z","iopub.execute_input":"2024-02-05T13:17:38.608175Z","iopub.status.idle":"2024-02-05T13:17:38.616259Z","shell.execute_reply.started":"2024-02-05T13:17:38.608142Z","shell.execute_reply":"2024-02-05T13:17:38.615444Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"generate_captions(\"1007320043_627395c3d8.jpg\")  \n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-05T13:19:03.347461Z","iopub.execute_input":"2024-02-05T13:19:03.347862Z","iopub.status.idle":"2024-02-05T13:19:03.422768Z","shell.execute_reply.started":"2024-02-05T13:19:03.347828Z","shell.execute_reply":"2024-02-05T13:19:03.421407Z"},"trusted":true},"execution_count":56,"outputs":[{"name":"stdout","text":"---------------Actual----------------------\nCaption 1\nCaption 2\nCaption 3\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[56], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mgenerate_captions\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m1007320043_627395c3d8.jpg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \n","Cell \u001b[0;32mIn[51], line 22\u001b[0m, in \u001b[0;36mgenerate_captions\u001b[0;34m(image_name)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(caption)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Assuming you have a function to predict captions\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m predict_caption(model, \u001b[43mfeatures\u001b[49m[image_id], tokenizer, max_length)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(y_pred)\n\u001b[1;32m     24\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(image)\n","\u001b[0;31mNameError\u001b[0m: name 'features' is not defined"],"ename":"NameError","evalue":"name 'features' is not defined","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}